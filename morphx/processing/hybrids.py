# -*- coding: utf-8 -*-
# MorphX - Toolkit for morphology exploration and segmentation
#
# Copyright (c) 2019 - now
# Max Planck Institute of Neurobiology, Martinsried, Germany
# Authors: Jonathan Klimesch, Philipp Schubert

import os
import pickle
import numpy as np
from morphx.classes.hybridcloud import HybridCloud
from morphx.classes.hybridmesh import HybridMesh
from morphx.classes.pointcloud import PointCloud
from morphx.classes.meshcloud import MeshCloud


# -------------------------------------- HYBRID EXTRACTION --------------------------------------- #

def extract_subset(hybrid: HybridCloud, local_bfs: np.ndarray) -> PointCloud:
    """ Returns the mesh subset of given skeleton nodes based on a mapping dict
     between skeleton and mesh.

    Args:
        hybrid: MorphX HybridCloud object from which the subset should be extracted.
        local_bfs: Skeleton node index array which was generated by a local BFS.

    Returns:
        Mesh subset as PointCloud object
    """
    idcs = []
    for i in local_bfs:
        idcs.extend(hybrid.verts2node[i])
    return PointCloud(hybrid.vertices[idcs], labels=hybrid.labels[idcs], features=hybrid.features[idcs],
                      no_pred=hybrid.no_pred, obj_bounds=hybrid.obj_bounds, encoding=hybrid.encoding)


def extract_mesh_subset(hm: HybridMesh, local_bfs: np.ndarray) -> MeshCloud:
    """ Returns the mesh subset of given skeleton nodes based on the face
     mapping dict between skeleton and mesh.

    Args:
        hm: HybridMesh from which the subset should be extracted.
        local_bfs: Skeleton node index array which was generated by a local BFS.

    Returns:
        Mesh subset as PointCloud object
    """
    mapping_face = hm.faces2node

    total_face = set()
    for node_ix in local_bfs:
        total_face.update(set(mapping_face[node_ix]))

    total_face = np.array(list(total_face), dtype=np.int)

    if len(total_face) == len(hm.faces):
        # all faces haven been selected
        new_faces = hm.faces
        new_vertices = hm.vertices
        new_labels = hm.labels
        new_normals = hm.normals
    else:
        new_faces = hm.faces[total_face]
        total_vertex = np.unique(new_faces.flatten()).astype(int)  # also sorts the ids
        new_vertices = hm.vertices[total_vertex]
        if len(hm.labels) > 0:
            new_labels = hm.labels[total_vertex]
        else:
            new_labels = None
        if len(hm.normals) > 0:
            new_normals = hm.normals[total_vertex]
        else:
            new_normals = None
        # normalize new faces to be contiguous
        face_shape = new_faces.shape
        new_faces = new_faces.flatten()
        updated_face_ixs = {face_id: k for k, face_id in enumerate(np.unique(new_faces))}
        for ix, old_face_ix in enumerate(new_faces):
            # update face indices
            new_faces[ix] = updated_face_ixs[old_face_ix]
        # switch to original shape
        new_faces = new_faces.reshape(face_shape)
    mc = MeshCloud(new_vertices, new_faces, new_normals, labels=new_labels,
                   encoding=hm.encoding)
    return mc


# -------------------------------------- Hybrid I/O ------------------------------------------- #


def hybrid_from_pkl(path):
    """ Loads a hybrid cloud from an existing pickle file.

    Args:
        path: File path of pickle file.
    """
    path = os.path.expanduser(path)
    if not os.path.exists(path):
        print(f"File with name: {path} was not found at this location.")
    with open(path, 'rb') as f:
        obj = pickle.load(f)
    f.close()

    return hybrid_from_attr_dict(obj)


def hybrid_from_attr_dict(attr_dict: dict):
    return HybridCloud(attr_dict['nodes'], attr_dict['edges'], attr_dict['vertices'],
                       labels=attr_dict['labels'], features=attr_dict['features'],
                       encoding=attr_dict['encoding'],
                       obj_bounds=attr_dict['obj_bounds'],
                       predictions=attr_dict['predictions'],
                       no_pred=attr_dict['no_pred'])
