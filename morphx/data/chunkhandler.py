# -*- coding: utf-8 -*-
# MorphX - Toolkit for morphology exploration and segmentation
#
# Copyright (c) 2019 - now
# Max Planck Institute of Neurobiology, Martinsried, Germany
# Authors: Jonathan Klimesch

import os
import glob
import pickle
import numpy as np
from scipy.spatial import cKDTree
from typing import Callable, Union, Tuple
from morphx.processing import clouds, hybrids
from morphx.preprocessing import splitting
from morphx.classes.pointcloud import PointCloud


class ChunkHandler:
    def __init__(self,
                 data_path: str,
                 chunk_size: int,
                 sample_num: int,
                 transform: Callable = clouds.Identity(),
                 specific: bool = False):
        self._data_path = os.path.expanduser(data_path)
        if not os.path.exists(self._data_path):
            os.makedirs(self._data_path)

        # Load chunks or split dataset into chunks if it was not done already
        if not os.path.exists(self._data_path + 'splitted/' + str(chunk_size) + '.pkl'):
            splitting.split(data_path, chunk_size)
        with open(self._data_path + 'splitted/' + str(chunk_size) + '.pkl', 'rb') as f:
            self._splitted_hcs = pickle.load(f)
        f.close()

        self._sample_num = sample_num
        self._transform = transform
        self._specific = specific

        # In non-specific mode, the entire dataset gets loaded
        self._hc_names = []
        self._hcs = []
        if not self._specific:
            files = glob.glob(data_path + '*.pkl')
            for file in files:
                slashs = [pos for pos, char in enumerate(file) if char == '/']
                name = file[slashs[-1] + 1:-4]
                self._hc_names.append(name)
                hc = clouds.load_cloud(file)
                self._hcs.append(hc)

        # In specific mode, the files should be loaded sequentially
        self._curr_hc = None
        self._curr_name = None

        # Index of current hc in self.hcs
        self._hc_idx = 0
        # Index of current chunk in current hc
        self._chunk_idx = 0
        # Size of entire dataset
        self._size = 0
        for name in self._hc_names:
            self._size += len(self._splitted_hcs[name])

    def __len__(self):
        return self._size

    def __getitem__(self, item: Union[int, Tuple[str, int]]):
        # Get specific item (e.g. chunk 5 of HybridCloud 1)
        if self._specific:
            if isinstance(item, tuple):
                splitted_hc = self._splitted_hcs[item[0]]

                # In specific mode, the files should be loaded sequentially
                if self._curr_name != item[0]:
                    self._curr_hc = clouds.load_cloud(self._data_path + item[0] + '.pkl')
                    self._curr_name = item[0]

                # Return PointCloud with zeros if requested chunk doesn't exist
                if item[1] >= len(splitted_hc) or abs(item[1]) > len(splitted_hc):
                    return PointCloud(np.zeros((self._sample_num, 3)), np.zeros(self._sample_num))

                # Load local BFS generated by splitter, extract vertices to all nodes of the local BFS (subset) and
                # draw random points of these vertices (sample)
                local_bfs = splitted_hc[item[1]]
                subset = hybrids.extract_cloud_subset(self._curr_hc, local_bfs)
                sample = clouds.sample_cloud(subset, self._sample_num)
            else:
                raise ValueError('In validation mode, items can only be requested with a tuple'
                                 'of HybridCloud name and chunk index within that cloud.')
        else:
            # Get the next item while iterating the entire dataset
            curr_hc_chunks = self._splitted_hcs[self._hc_names[self._hc_idx]]
            if self._chunk_idx >= len(curr_hc_chunks):
                self._hc_idx += 1
                if self._hc_idx >= len(self._hcs):
                    self._hc_idx = 0
                self._chunk_idx = 0

            # Load local BFS generated by splitter, extract vertices to all nodes of the local BFS (subset) and draw
            # random points of these vertices (sample)
            local_bfs = curr_hc_chunks[self._chunk_idx]
            subset = hybrids.extract_cloud_subset(self._hcs[self._hc_idx], local_bfs)
            sample = clouds.sample_cloud(subset, self._sample_num)
            self._chunk_idx += 1

        # Apply transformations (e.g. Composition of Rotation and Normalization)
        if len(sample.vertices) > 0:
            self._transform(sample)

        return sample

    def set_specific_mode(self, specific: bool):
        self._specific = specific

    def get_hybrid_length(self, name: str):
        return len(self._splitted_hcs[name])

    def map_predictions(self, pred_cloud: PointCloud, hybrid_name: str, chunk_idx: int, save_path: str):
        save_path = os.path.expanduser(save_path)

        # If requested hybrid differs from hybrid in memory, save current hybrid and try loading new hybrid from save
        # path in case previous predictions were already saved before. If that fails, load new hybrid from data path
        if self._curr_name != hybrid_name:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            clouds.save_cloud(self._curr_hc, save_path, name=hybrid_name)
            try:
                self._curr_hc = clouds.load_cloud(save_path + hybrid_name + '.pkl')
            except FileNotFoundError:
                self._curr_hc = clouds.load_cloud(self._data_path + hybrid_name + '.pkl')
            self._curr_name = hybrid_name

        local_bfs = self._splitted_hcs[hybrid_name][chunk_idx]

        # Get indices of vertices for requested local BFS
        idcs = []
        for i in local_bfs:
            idcs.extend(self._curr_hc.verts2node[i])

        # Apply invers transformations to compare the predicted cloud with the original cloud
        if len(pred_cloud.vertices) > 0:
            self._transform(pred_cloud, invers=True)

        # Vertices of predicted cloud can differ from the original ones as sampling is altering the order and may add
        # additional points
        tree = cKDTree(self._curr_hc.vertices[idcs])
        dist, ind = tree.query(pred_cloud.vertices, k=1)
        for pred_idx, vertex_idx in enumerate(ind):
            self._curr_hc.predictions[vertex_idx].append(pred_cloud.labels[pred_idx])
