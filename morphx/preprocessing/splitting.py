# -*- coding: utf-8 -*-
# MorphX - Toolkit for morphology exploration and segmentation
#
# Copyright (c) 2020 - now
# Max Planck Institute of Neurobiology, Martinsried, Germany
# Authors: Jonathan Klimesch

import os
import glob
import pickle
import numpy as np
from tqdm import tqdm
from morphx.processing import ensembles, objects, graphs


def split(data_path: str, filename: str, bio_density: float = None, capacity: int = None, tech_density: int = None,
          density_mode: bool = True, chunk_size: int = None):
    """ Splits HybridClouds given as pickle files at data_path into multiple chunks and saves that chunking information
        in the new folder 'splitted' as a pickled dict. The dict has filenames of the HybridClouds as keys and lists of
        chunks as values. The chunks are saved as numpy arrays of the indices of skeleton nodes which belong to the
        respective chunk.

    Args:
        data_path: Path to HybridClouds saved as pickle files.
        filename: the file in which splitting information should get saved.
        tech_density: poisson sampling density with which data set was preprocessed in point/um²
        bio_density: chunk sampling density in point/um²
        capacity: number of points which can get processed with given network architecture
        density_mode: Flag for switching between density and context modes
        chunk_size: Is only used in context mode. Here, the chunks get generated by a certain size.
    """
    data_path = os.path.expanduser(data_path)
    files = glob.glob(data_path + '*.pkl')
    splitted_hcs = {}
    print("No splitting information exists for the given chunk size. Splitting of dataset is required.")
    for file in tqdm(files):
        slashs = [pos for pos, char in enumerate(file) if char == '/']
        name = file[slashs[-1] + 1:-4]
        obj = ensembles.ensemble_from_pkl(file)
        # calculate number of vertices for extracting max surface area
        vert_num = int(capacity * tech_density / bio_density)
        base_points = []
        chunks = []
        nodes_new = obj.nodes
        # choose random node, extract local context at this point, remove local context nodes, repeat until empty
        while len(nodes_new) != 0:
            choice = np.random.choice(len(nodes_new), 1)
            base_points.append(choice[0])
            if density_mode:
                if bio_density is None or tech_density is None or capacity is None:
                    raise ValueError('bio_density, tech_density and capacity must be given in density mode')
                bfs = objects.bfs_vertices_diameter(obj, choice[0], vert_num)
            else:
                if chunk_size is None:
                    raise ValueError('chunk_size parameter must be given in context mode.')
                bfs = graphs.bfs_euclid(obj.graph(), choice[0], chunk_size)
            chunks.append(bfs)
            mask = np.ones(len(nodes_new), dtype=bool)
            mask[bfs] = False
            nodes_new = nodes_new[mask]
        base_points = np.array(base_points)

        slashs = [pos for pos, char in enumerate(filename) if char == '/']
        identifier = filename[slashs[-1] + 1:-4]
        basefile = f'{filename[:slashs[-1]]}/base_points/{identifier}/'
        if not os.path.exists(basefile):
            os.makedirs(basefile)
        with open(f'{basefile}{name}_basepoints.pkl', 'wb') as f:
            pickle.dump(base_points, f)
        splitted_hcs[name] = chunks
    with open(filename, 'wb') as f:
        pickle.dump(splitted_hcs, f)
    f.close()
